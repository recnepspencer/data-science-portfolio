[
  {
    "objectID": "ds250_project_template_clean (3).html",
    "href": "ds250_project_template_clean (3).html",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "Our project leverages advanced analytics to illuminate the relationship between a Major League Baseball team’s performance and its financial strategy. We’ve developed a model that specifically examines how the previous year’s home run statistics predict the following year’s salary allocations for the Boston Red Sox and New York Yankees. Our findings provide strategic insights that can guide team investments and roster decisions, utilizing historical data to forecast salary trends with significant precision. This approach empowers teams to optimize their spending for maximum on-field effectiveness, offering a competitive edge in the league’s dynamic environment.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\nresults\n\nq = '''\n    SELECT * \n    FROM sqlite_master \n    WHERE type='table'\n    '''\ntable = pd.read_sql_query(q,con)"
  },
  {
    "objectID": "ds250_project_template_clean (3).html#elevator-pitch",
    "href": "ds250_project_template_clean (3).html#elevator-pitch",
    "title": "Client Report - Project 3",
    "section": "",
    "text": "Our project leverages advanced analytics to illuminate the relationship between a Major League Baseball team’s performance and its financial strategy. We’ve developed a model that specifically examines how the previous year’s home run statistics predict the following year’s salary allocations for the Boston Red Sox and New York Yankees. Our findings provide strategic insights that can guide team investments and roster decisions, utilizing historical data to forecast salary trends with significant precision. This approach empowers teams to optimize their spending for maximum on-field effectiveness, offering a competitive edge in the league’s dynamic environment.\n\n\nRead and format project data\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\nresults\n\nq = '''\n    SELECT * \n    FROM sqlite_master \n    WHERE type='table'\n    '''\ntable = pd.read_sql_query(q,con)"
  },
  {
    "objectID": "ds250_project_template_clean (3).html#questiontask-1",
    "href": "ds250_project_template_clean (3).html#questiontask-1",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nHere we discover that 2 different people in the database attended BYU-Idaho.\n\n\nShow the code\nbyui_salary_query = \"\"\"\nSELECT p.nameFirst as 'First Name', p.nameLast as 'Last Name', cp.playerID, cp.schoolID, s.salary, s.yearID, s.teamID\nFROM collegeplaying AS cp\nJOIN salaries AS s ON cp.playerID = s.playerID\nJOIN people AS p ON cp.playerID = p.playerID\nWHERE cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC;\n\"\"\"\n\n# Use pandas to run the SQL query and create a DataFrame\ndf = pd.read_sql_query(byui_salary_query, con)\n\n# Print the DataFrame\nprint(df)\n\n\n   First Name   Last Name   playerID schoolID     salary  yearID teamID\n0        Matt   Lindstrom  lindsma01  idbyuid  4000000.0    2014    CHA\n1        Matt   Lindstrom  lindsma01  idbyuid  4000000.0    2014    CHA\n2        Matt   Lindstrom  lindsma01  idbyuid  3600000.0    2012    BAL\n3        Matt   Lindstrom  lindsma01  idbyuid  3600000.0    2012    BAL\n4        Matt   Lindstrom  lindsma01  idbyuid  2800000.0    2011    COL\n5        Matt   Lindstrom  lindsma01  idbyuid  2800000.0    2011    COL\n6        Matt   Lindstrom  lindsma01  idbyuid  2300000.0    2013    CHA\n7        Matt   Lindstrom  lindsma01  idbyuid  2300000.0    2013    CHA\n8        Matt   Lindstrom  lindsma01  idbyuid  1625000.0    2010    HOU\n9        Matt   Lindstrom  lindsma01  idbyuid  1625000.0    2010    HOU\n10    Garrett  Stephenson  stephga01  idbyuid  1025000.0    2001    SLN\n11    Garrett  Stephenson  stephga01  idbyuid  1025000.0    2001    SLN\n12    Garrett  Stephenson  stephga01  idbyuid   900000.0    2002    SLN\n13    Garrett  Stephenson  stephga01  idbyuid   900000.0    2002    SLN\n14    Garrett  Stephenson  stephga01  idbyuid   800000.0    2003    SLN\n15    Garrett  Stephenson  stephga01  idbyuid   800000.0    2003    SLN\n16    Garrett  Stephenson  stephga01  idbyuid   550000.0    2000    SLN\n17    Garrett  Stephenson  stephga01  idbyuid   550000.0    2000    SLN\n18       Matt   Lindstrom  lindsma01  idbyuid   410000.0    2009    FLO\n19       Matt   Lindstrom  lindsma01  idbyuid   410000.0    2009    FLO\n20       Matt   Lindstrom  lindsma01  idbyuid   395000.0    2008    FLO\n21       Matt   Lindstrom  lindsma01  idbyuid   395000.0    2008    FLO\n22       Matt   Lindstrom  lindsma01  idbyuid   380000.0    2007    FLO\n23       Matt   Lindstrom  lindsma01  idbyuid   380000.0    2007    FLO\n24    Garrett  Stephenson  stephga01  idbyuid   215000.0    1999    SLN\n25    Garrett  Stephenson  stephga01  idbyuid   215000.0    1999    SLN\n26    Garrett  Stephenson  stephga01  idbyuid   185000.0    1998    PHI\n27    Garrett  Stephenson  stephga01  idbyuid   185000.0    1998    PHI\n28    Garrett  Stephenson  stephga01  idbyuid   150000.0    1997    PHI\n29    Garrett  Stephenson  stephga01  idbyuid   150000.0    1997    PHI"
  },
  {
    "objectID": "ds250_project_template_clean (3).html#questiontask-2",
    "href": "ds250_project_template_clean (3).html#questiontask-2",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\n\n\n\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\n\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\n\n\n\nPart 1\n\n\n\nShow the code\nat_least_one_bat_query = \"\"\"\nSELECT playerID, yearID, CAST(H AS FLOAT) / AB AS battingAverage\nFROM Batting\nWHERE AB &gt;= 1\nORDER BY battingAverage DESC, playerID\nLIMIT 5;\n\"\"\"\n\ndf = pd.read_sql_query(at_least_one_bat_query, con)\nprint(df)\n\n\n    playerID  yearID  battingAverage\n0   aberal01    1957             1.0\n1  abernte02    1960             1.0\n2  abramge01    1923             1.0\n3  acklefr01    1964             1.0\n4  alanirj01    2019             1.0\n\n\n\nPart 2\n\n\n\nShow the code\nat_least_ten_bats_query = \"\"\"\nSELECT playerID, yearID, CAST(H AS FLOAT) / AB AS battingAverage\nFROM Batting\nWHERE AB &gt;= 10\nORDER BY battingAverage DESC, playerID\nLIMIT 5;\n\"\"\"\n\ndf = pd.read_sql_query(at_least_ten_bats_query, con)\nprint(df)\n\n\n    playerID  yearID  battingAverage\n0  nymanny01    1974        0.642857\n1  carsoma01    2013        0.636364\n2  altizda01    1910        0.600000\n3  johnsde01    1975        0.600000\n4  silvech01    1948        0.571429\n\n\n\nPart 3\n\n\n\nShow the code\nat_least_hundo_bats_query = \"\"\"\nSELECT playerID, SUM(H) AS totalHits, SUM(AB) AS totalAtBats, CAST(SUM(H) AS FLOAT) / SUM(AB) AS careerBattingAverage\nFROM Batting\nGROUP BY playerID\nHAVING SUM(AB) &gt;= 100\nORDER BY careerBattingAverage DESC, playerID\nLIMIT 5;\n\"\"\"\n\ndf = pd.read_sql_query(at_least_hundo_bats_query, con)\nprint(df)\n\n\n    playerID  totalHits  totalAtBats  careerBattingAverage\n0   cobbty01       4189        11436              0.366299\n1  barnero01        860         2391              0.359682\n2  hornsro01       2930         8173              0.358497\n3  jacksjo01       1772         4981              0.355752\n4  meyerle01        513         1443              0.355509"
  },
  {
    "objectID": "ds250_project_template_clean (3).html#questiontask-3",
    "href": "ds250_project_template_clean (3).html#questiontask-3",
    "title": "Client Report - Project 3",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nWe used the historical CPI data to get a good metric for inflation and added to the database. Then we calculated the increase in home runs compared to last year as well as the change in salary compared to last year. \n\n\nAdd the inflation data and make the sql query\ncon = sqlite3.connect('lahmansbaseballdb.sqlite')\ncreate_table_stmt = \"\"\"\nCREATE TABLE IF NOT EXISTS InflationRates (\n    Year INTEGER PRIMARY KEY,\n    Inflation_Rate REAL\n);\n\"\"\"\ncon.execute(create_table_stmt)\n\n# Inflation data\ninflation_data = {\n    'Year': list(range(1983, 2024)),\n    'Inflation_Rate': [\n        3.2, 4.3, 3.6, 1.9, 3.6, 4.1, 4.8, 5.4, 4.2, 3.0, 3.0,\n        2.6, 2.8, 3.0, 2.3, 1.6, 2.2, 3.4, 2.8, 1.6, 2.3, 2.7,\n        3.4, 3.2, 2.8, 3.8, -0.4, 1.6, 3.2, 2.1, 1.5, 1.6, 0.1,\n        1.3, 2.1, 2.4, 1.8, 1.2, 4.7, 8.0, 4.1\n    ]\n}\n\n# Prepare SQL insert statement\ninsert_stmt = 'INSERT OR IGNORE INTO InflationRates (Year, Inflation_Rate) VALUES (?, ?)'\n\n# Insert each row of data\nfor year, rate in zip(inflation_data['Year'], inflation_data['Inflation_Rate']):\n    con.execute(insert_stmt, (year, rate))\n\n# Commit changes and close connection\ncon.commit()\n\nhome_runs_over_salary_query = \"\"\"\nWITH TeamStats AS (\n    SELECT \n        b.yearID,\n        b.teamID,\n        SUM(b.HR) AS totalHRs,\n        SUM(s.salary) AS totalSalary\n    FROM Batting b\n    JOIN Salaries s ON b.playerID = s.playerID AND b.yearID = s.yearID\n    GROUP BY b.yearID, b.teamID\n),\nInflationAdjusted AS (\n    SELECT \n        ts.yearID,\n        ts.teamID,\n        ts.totalHRs,\n        ts.totalSalary,\n        LAG(ts.totalHRs) OVER(PARTITION BY ts.teamID ORDER BY ts.yearID) AS lastYearHRs,\n        LAG(ts.totalSalary) OVER(PARTITION BY ts.teamID ORDER BY ts.yearID) AS lastYearSalary,\n        ir.Inflation_Rate\n    FROM TeamStats ts\n    JOIN InflationRates ir ON ts.yearID = ir.Year\n),\nSalaryHRComparison AS (\n    SELECT \n        yearID,\n        teamID,\n        totalHRs,\n        totalSalary,\n        lastYearHRs,\n        lastYearSalary,\n        totalHRs - lastYearHRs AS HR_Difference,\n        (totalSalary - (lastYearSalary * (1 + Inflation_Rate / 100))) AS SalaryDiff_AdjustedForInflation\n    FROM InflationAdjusted\n)\nSELECT \n    yearID,\n    teamID,\n    totalHRs,\n    totalSalary,\n    lastYearHRs,\n    lastYearSalary,\n    HR_Difference,\n    SalaryDiff_AdjustedForInflation\nFROM SalaryHRComparison\nWHERE HR_Difference &gt; 0\nORDER BY teamID, yearID;\n\n\"\"\"\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nHere we calculate the startistics for our graph\ndf = pd.read_sql_query(home_runs_over_salary_query, con)\n\nX = sm.add_constant(df['HR_Difference'])\ny = df['SalaryDiff_AdjustedForInflation'] \n\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary())\n\ndf['predictions'] = model.predict(X)\n\n\n                                   OLS Regression Results                                  \n===========================================================================================\nDep. Variable:     SalaryDiff_AdjustedForInflation   R-squared:                       0.001\nModel:                                         OLS   Adj. R-squared:                 -0.001\nMethod:                              Least Squares   F-statistic:                    0.4388\nDate:                             Fri, 05 Apr 2024   Prob (F-statistic):              0.508\nTime:                                     16:36:18   Log-Likelihood:                -8118.5\nNo. Observations:                              451   AIC:                         1.624e+04\nDf Residuals:                                  449   BIC:                         1.625e+04\nDf Model:                                        1                                         \nCovariance Type:                         nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst          3.172e+06   1.26e+06      2.511      0.012    6.89e+05    5.66e+06\nHR_Difference  2.319e+04    3.5e+04      0.662      0.508   -4.56e+04     9.2e+04\n==============================================================================\nOmnibus:                       56.140   Durbin-Watson:                   2.104\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              346.786\nSkew:                           0.276   Prob(JB):                     4.97e-76\nKurtosis:                       7.260   Cond. No.                         60.8\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n::: {#cell-Q3 Graph .cell execution_count=9}\n\nHere we graph the results of our data\n# Initialize a Plotly figure\nfig = go.Figure()\n\n# Colors for the teams\ncolors = {\n    'BOS': 'Crimson',\n    'NYA': 'RoyalBlue'\n}\n\n# Mapping the teamID to the full team name for display\nteam_names = {\n    'BOS': 'Red Sox',\n    'NYA': 'Yankees'\n}\n\n# Loop through each team to plot\nfor team, color in colors.items():\n    # Filter the DataFrame for the current team\n    team_df = df[df['teamID'] == team]  # Assuming SettingWithCopyWarning is not an issue here\n    \n    # Assuming the correct column names are 'HR_Difference' and 'SalaryDiff_AdjustedForInflation'\n    # Prepare the data for regression\n    X_team = sm.add_constant(team_df['HR_Difference'])  # Predictor with constant added\n    y_team = team_df['SalaryDiff_AdjustedForInflation']  # Response variable\n    \n    # Perform linear regression\n    model_team = sm.OLS(y_team, X_team).fit()\n    \n    # Store predictions for plotting\n    team_df = team_df.assign(predictions=model_team.predict(X_team))\n    \n    # Plotting Actual Data Points with customized hover text\n    fig.add_trace(go.Scatter(\n        x=team_df['HR_Difference'], y=team_df['SalaryDiff_AdjustedForInflation'],\n        mode='markers', name=f'{team_names[team]}',\n        marker=dict(color=color),\n        text=team_df['yearID'],  # Add the year to the hover information\n        hovertemplate=f\"&lt;b&gt;{team_names[team]}&lt;/b&gt;&lt;br&gt;\" +\n                      \"%{x} more home runs than last year.&lt;br&gt;\" +\n                      \"Salary change: %{y} &lt;br&gt;\" +\n                      \"Year: %{text}&lt;br&gt;\" +\n                      \"&lt;extra&gt;&lt;/extra&gt;\"  # Removes the trace name from the hover text\n    ))\n    \n    # Plotting Regression Line\n    fig.add_trace(go.Scatter(\n        x=team_df['HR_Difference'], y=team_df['predictions'],\n        mode='lines', name=f'{team_names[team]} Regression Line',\n        line=dict(color=color)\n    ))\n\n    team_index = list(colors.keys()).index(team)\n    fig.add_annotation(\n        xref=\"paper\", yref=\"paper\", x=0.05, y=1 - (0.08 * (team_index + 1)),  # Adjust y for each team\n        text=f\"{team_names[team]} p-value: {model_team.pvalues[1]:.4f}\",\n        showarrow=False,\n        font=dict(size=12, color=color),\n        bgcolor=\"rgba(255,255,255,0.9)\",\n        bordercolor=color,\n        borderwidth=1,\n        align=\"left\"\n)\n\n# Updating the layout of the plot\nfig.update_layout(\n    title='Regression: Does hitting more home runs lead to a team salary increase?',\n    xaxis_title='Year-over-Year HR Difference',\n    yaxis_title='Inflation-Adjusted Salary Increase',\n    legend_title='Legend',\n    hovermode='closest'\n)\n\nfig.show()\n\n\n                                                \n\n:::\n\nHere we discover that there is not enough evidence to conclude that there is a relationship between hitting more home runs and a team’s salary increase. In fact, the p-value is very high, which means that it is extremely unlikely that there is a liner correlation in the data."
  },
  {
    "objectID": "ds250_project_template_clean (2).html",
    "href": "ds250_project_template_clean (2).html",
    "title": "Client Report - Project 2",
    "section": "",
    "text": "Show the code\nimport requests\nimport datetime\nimport certifi\nimport pandas as pd\nimport io\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport numpy as np"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#elevator-pitch",
    "href": "ds250_project_template_clean (2).html#elevator-pitch",
    "title": "Client Report - Project 2",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nWe discovered that different airports have different levels of delays. There are different aspects to consider, however. For example, in busy months, delays increase across the board. Therefore it is possible that high passenger volume is responsible for a large protion of the delays. There are other considerations too, such as weather and the operations of each specific airport.\n\n\nRead and format project data\n# Include and execute your code here\n# URL of the CSV data\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json'\n\nresponse = requests.get(url, verify=certifi.where())\nif response.status_code == 200:\n    df = pd.read_json(io.StringIO(response.content.decode('utf-8')))\nelse:\n    print(\"Failed to retrieve data: Status code\", response.status_code)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#questiontask-1",
    "href": "ds250_project_template_clean (2).html#questiontask-1",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”).\nI was able to successfuly address the missing data in appropriate ways. However, further steps could be taken, such as a more statistical approach to the 1500+ issue. Also, decuction could be used to fill in data for the missing months and years.\n\n\nRead and format data\n# Include and execute your code here\ndf.replace(-999, \"NaN\", inplace=True) \ndf.replace(\"Febuary\", \"February\", inplace=True)\ndf.replace(\"1500+\", 1500, inplace=True)\ndf.fillna(\"NaN\", inplace=True)\ndf.replace('', 'NaN', inplace=True)\nnum_missing = (df == \"NaN\").sum().sum()  # This counts all \nprint(f\"Number of 'NaN' strings in the DataFrame: {num_missing}\")\nexample_row_json = df.iloc[0].to_json()\nprint(\"Example row in JSON format with 'NaN':\")\nprint(example_row_json)\n\n\nNumber of 'NaN' strings in the DataFrame: 219\nExample row in JSON format with 'NaN':\n{\"airport_code\":\"ATL\",\"airport_name\":\"Atlanta, GA: Hartsfield-Jackson Atlanta International\",\"month\":\"January\",\"year\":2005.0,\"num_of_flights_total\":35048,\"num_of_delays_carrier\":1500,\"num_of_delays_late_aircraft\":\"NaN\",\"num_of_delays_nas\":4598,\"num_of_delays_security\":10,\"num_of_delays_weather\":448,\"num_of_delays_total\":8355,\"minutes_delayed_carrier\":116423.0,\"minutes_delayed_late_aircraft\":104415,\"minutes_delayed_nas\":207467.0,\"minutes_delayed_security\":297,\"minutes_delayed_weather\":36931,\"minutes_delayed_total\":465533}"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#questiontask-2",
    "href": "ds250_project_template_clean (2).html#questiontask-2",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\nChicago and San Francisco are among the worst-run cities in the United States. Unsurprisingly, the data shows that they have the worst delays, both in length and proportion.\n\n\nRead and format data\n# Include and execute your code here\ndf['num_of_delays_carrier'] = pd.to_numeric(df['num_of_delays_carrier'])\ndf['num_of_delays_late_aircraft'] = pd.to_numeric(df['num_of_delays_late_aircraft'], errors='coerce')\n\n# Aggregate data by airport\nagg_funcs = {\n    'num_of_flights_total': 'sum',\n    'num_of_delays_total': 'sum',\n    'minutes_delayed_total': 'sum'\n}\nairport_summary = df.groupby('airport_code').agg(agg_funcs)\n\n# Calculate additional metrics\nairport_summary['proportion_of_delays'] = airport_summary['num_of_delays_total'] / airport_summary['num_of_flights_total']\nairport_summary['average_delay_time_minutes'] = airport_summary['minutes_delayed_total'] / airport_summary['num_of_delays_total']\n\n# Convert average delay time to hours for readability\nairport_summary['average_delay_time_hours'] = airport_summary['average_delay_time_minutes'] / 60\n\n# Sorting airports by the highest average delay time\nsorted_airports = airport_summary.sort_values(by='average_delay_time_hours', ascending=False)\n\n# Display the top airports with the worst average delay times\nprint(sorted_airports[['num_of_flights_total', 'num_of_delays_total', 'proportion_of_delays', 'average_delay_time_hours']])\n\n\n              num_of_flights_total  num_of_delays_total  proportion_of_delays  \\\nairport_code                                                                    \nORD                        3597588               830825              0.230939   \nSFO                        1630945               425604              0.260955   \nIAD                         851571               168467              0.197831   \nATL                        4430047               902443              0.203710   \nDEN                        2513974               468519              0.186366   \nSLC                        1403384               205160              0.146189   \nSAN                         917862               175132              0.190804   \n\n              average_delay_time_hours  \nairport_code                            \nORD                           1.130525  \nSFO                           1.039718  \nIAD                           1.017358  \nATL                           0.996996  \nDEN                           0.895495  \nSLC                           0.822396  \nSAN                           0.787620  \n\n\nHere we discover see which airports have the longest delays, and which ones have the greatest ratio of flights that get delayed. Surprisingly, the ones with the shortest delays are also the ones with the least amount of delays altogether. \n::: {#cell-Q2 chart .cell execution_count=5}\n\nplot of flight delays\n# Include and execute your code here\ntrace1 = go.Bar(\n    x=airport_summary.index,\n    y=airport_summary['average_delay_time_hours'],\n    name='Average Delay Time (Hours)',\n    marker=dict(color='skyblue'),\n    # Format to 2 decimal places for the labels\n    text=airport_summary['average_delay_time_hours'].apply(lambda x: f'{x:.2f}'),\n    textposition='auto'\n)\n\ntrace2 = go.Bar(\n    x=airport_summary.index,\n    y=airport_summary['proportion_of_delays'],\n    name='Proportion of Delays',\n    marker=dict(color='lightcoral'),\n    # Already rounded to 2 decimal places, ensure consistent formatting\n    text=airport_summary['proportion_of_delays'].apply(lambda x: f'{x:.2f}'),\n    textposition='auto'\n)\n\n# Combining the traces\ndata = [trace1, trace2]\n\n# Updating layout for a clearer view\nlayout = go.Layout(\n    title='Airport Delays: Average Delay Time vs. Proportion of Delays',\n    width=800,\n    xaxis=dict(title='Airport Code'),\n    yaxis=dict(title='Values', tickformat=\".2f\"),\n    barmode='group'  # This will group bars for the same airport side by side\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n# Show the figure\nfig.show()\n\n\n                                                \nAnalysis of airport delays in 7 different airpoirts over a several year time span\n\n:::"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#questiontask-3",
    "href": "ds250_project_template_clean (2).html#questiontask-3",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\nIt looks like the best time to fly is after the school year starts but before the holidays begin. It is likely due to the fact it is not busy and there are also not too many crazy weather events during that time. December, which is known for being busy and having bad weather, is the worst month.\n\n\nRead and format data\n# Include and execute your code here\n\nvalid_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\ndf_filtered = df[df['month'].isin(valid_months)]\n\n# Group by 'month' to aggregate the data\nmonthly_totals = df_filtered.groupby('month').agg(\n    total_delays=('num_of_delays_total', 'sum'),\n    total_flights=('num_of_flights_total', 'sum')\n).reset_index()\n\n# Calculate the proportion of delayed flights for each month\nmonthly_totals['proportion_of_delays'] = monthly_totals['total_delays'] / monthly_totals['total_flights']\n\n# Ensure the months are in chronological order for plotting\nmonthly_order = {month: index for index, month in enumerate(valid_months, start=1)}\nmonthly_totals['month_order'] = monthly_totals['month'].map(monthly_order)\nmonthly_totals.sort_values('month_order', inplace=True)\n\n\n::: {#cell-Q3 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nfig = px.bar(\n    monthly_totals, \n    x='month', \n    y='proportion_of_delays',\n    title='Proportion of Delayed Flights by Month',\n    labels={'proportion_of_delays': 'Proportion of Delays', 'month': 'Month'},\n    text='proportion_of_delays'\n)\n\nfig.update_layout(width=800, height=600)\n\nfig.for_each_trace(\n    lambda trace: trace.update(x=[month for month in trace.x if month not in ['n/a', 'NaN']])\n)\n\n# Show the figure\nfig.show()\n\n\n                                                \nMy useless chart\n\n:::"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#questiontask-4",
    "href": "ds250_project_template_clean (2).html#questiontask-4",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nCreate a new column that calculates the total number of flights delayed by weather (both severe and mild).\n\n\nRead and format data\n# Include and execute your code here\n\nmean_late_aircraft_delays = df['num_of_delays_late_aircraft'].replace(\"NaN\", np.nan).astype(float).mean()\ndf['num_of_delays_late_aircraft'] = df['num_of_delays_late_aircraft'].replace(\"NaN\", mean_late_aircraft_delays).astype(float)\n\n# Define a function to calculate weather delays based on the month\ndef calculate_weather_delays(row):\n    nas_weather_delays = row['num_of_delays_nas'] * 0.4 if row['month'] in ['April', 'May', 'June', 'July', 'August'] else row['num_of_delays_nas'] * 0.65\n    late_aircraft_weather_delays = row['num_of_delays_late_aircraft'] * 0.3\n    total_weather_delays = row['num_of_delays_weather'] + nas_weather_delays + late_aircraft_weather_delays\n    return total_weather_delays\n\n\ndf['total_weather_delays'] = df.apply(calculate_weather_delays, axis=1)\n\ndf.head()\n\n\n\n\n\n\n\n\n\n\nairport_code\nairport_name\nmonth\nyear\nnum_of_flights_total\nnum_of_delays_carrier\nnum_of_delays_late_aircraft\nnum_of_delays_nas\nnum_of_delays_security\nnum_of_delays_weather\nnum_of_delays_total\nminutes_delayed_carrier\nminutes_delayed_late_aircraft\nminutes_delayed_nas\nminutes_delayed_security\nminutes_delayed_weather\nminutes_delayed_total\ntotal_weather_delays\n\n\n\n\n0\nATL\nAtlanta, GA: Hartsfield-Jackson Atlanta Intern...\nJanuary\n2005.0\n35048\n1500\nNaN\n4598\n10\n448\n8355\n116423.0\n104415\n207467.0\n297\n36931\n465533\nNaN\n\n\n1\nDEN\nDenver, CO: Denver International\nJanuary\n2005.0\n12687\n1041\n928.0\n935\n11\n233\n3153\n53537.0\n70301\n36817.0\n363\n21779\n182797\n1119.15\n\n\n2\nIAD\nNaN\nJanuary\n2005.0\n12381\n414\n1058.0\n895\n4\n61\n2430\nNaN\n70919\n35660.0\n208\n4497\n134881\n960.15\n\n\n3\nORD\nChicago, IL: Chicago O'Hare International\nJanuary\n2005.0\n28194\n1197\n2255.0\n5415\n5\n306\n9178\n88691.0\n160811\n364382.0\n151\n24859\n638894\n4502.25\n\n\n4\nSAN\nSan Diego, CA: San Diego International\nJanuary\n2005.0\n7283\n572\n680.0\n638\n7\n56\n1952\n27436.0\n38445\n21127.0\n218\n4326\n91552\n674.70"
  },
  {
    "objectID": "ds250_project_template_clean (2).html#questiontask-5",
    "href": "ds250_project_template_clean (2).html#questiontask-5",
    "title": "Client Report - Project 2",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nUnsurprisingly, Chicago and San Francisco, which both have a lot of weather event, have the most delays from weather..\n\n\nRead and format data\n# Include and execute your code here\nairport_totals = df.groupby('airport_name').agg(\n    total_weather_delays=('total_weather_delays', 'sum'),\n    total_flights=('num_of_flights_total', 'sum')\n).reset_index()\n\n\nairport_totals['proportion_weather_delays'] = airport_totals['total_weather_delays'] / airport_totals['total_flights']\n\n\n# Create a bar plot using Plotly Express\nfig = px.bar(\n    airport_totals, \n    x='airport_name', \n    y='proportion_weather_delays', \n    title='Proportion of Flights Delayed by Weather at Each Airport',\n    labels={'proportion_weather_delays': 'Proportion of Weather Delays', 'airport_name': 'Airport'}\n)\n\nfig.update_layout(width=800, height=600)\n\nfig.show()"
  },
  {
    "objectID": "project-5.html",
    "href": "project-5.html",
    "title": "Do Star Wars preferences predict income?",
    "section": "",
    "text": "Asking whether or not star wars preferences are predictive of income is an interesting question. I went in with the hypothesis that the things from the survey that would have the most impact in predicting would be the demographic information, which turned out to be true. The Star wars preferences had very little impact on the outcome of the data.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv'\n\n# Securely fetch the data using requests and certifi\nresponse = requests.get(url, verify=certifi.where())\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    try:\n        data = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n    except UnicodeDecodeError:\n        data = pd.read_csv(io.StringIO(response.content.decode('ISO-8859-1')))\nelse:\n    print(\"Failed to retrieve data: Status code\", response.status_code)\n\nssl._create_default_https_context = ssl._create_unverified_context"
  },
  {
    "objectID": "project-5.html#elevator-pitch",
    "href": "project-5.html#elevator-pitch",
    "title": "Do Star Wars preferences predict income?",
    "section": "",
    "text": "Asking whether or not star wars preferences are predictive of income is an interesting question. I went in with the hypothesis that the things from the survey that would have the most impact in predicting would be the demographic information, which turned out to be true. The Star wars preferences had very little impact on the outcome of the data.\n\n\nRead and format project data\n# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html\n\n# Include and execute your code here\nurl = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/star-wars-survey/StarWars.csv'\n\n# Securely fetch the data using requests and certifi\nresponse = requests.get(url, verify=certifi.where())\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    try:\n        data = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n    except UnicodeDecodeError:\n        data = pd.read_csv(io.StringIO(response.content.decode('ISO-8859-1')))\nelse:\n    print(\"Failed to retrieve data: Status code\", response.status_code)\n\nssl._create_default_https_context = ssl._create_unverified_context"
  },
  {
    "objectID": "project-5.html#cleaning-the-data",
    "href": "project-5.html#cleaning-the-data",
    "title": "Do Star Wars preferences predict income?",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nFirst, we will use natural language processing as a good baseline for renaming the columns.\nIt is good generic code, but we will still need to make quite a few adjustments\n\n\nWe methodically came up with a way to pick out nouns, verbs, and adjectives to turn them into column names.\n# Include and execute your code here\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nssl._create_default_https_context = ssl._create_unverified_context\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\ncolumns = data.columns\n\nstate_of_being_verbs = {'is', 'are', 'was', 'were', 'be', 'being', 'been', 'do'}\noverride_words_to_include = {'shot', 'fan'} \noverride_words_to_remove = {'please', 'select', 'apply'}\n\ndef extract_relevant_words(text):\n    tokens = word_tokenize(text)\n    tagged = pos_tag(tokens)\n\n    first_adjective = ''\n    relevant_words = []\n\n    for word, tag in tagged:\n        word_lower = word.lower()\n\n        if tag.startswith('JJ') and not first_adjective:\n            first_adjective = word\n        elif word_lower in override_words_to_include:\n            relevant_words.append(word)\n        elif (tag.startswith('VB') and word_lower not in state_of_being_verbs) or tag == 'NNP' or tag == 'CD':\n            if word_lower not in override_words_to_remove:\n                relevant_words.append(word)\n\n    if first_adjective:\n        relevant_words.insert(0, first_adjective)\n\n    if not relevant_words and tokens:\n        return tokens[0]\n\n    return ''.join(relevant_words)\n\n# Apply the function to all column names and create a mapping\nmodified_column_names = [extract_relevant_words(name) for name in columns]\ncolumn_names_mapping = dict(zip(columns, modified_column_names))\n\n# Rename the columns using the generated mapping\ndata.rename(columns=column_names_mapping, inplace=True)\n\n\n[nltk_data] Downloading package punkt to /Users/spenstar/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/spenstar/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n\n\nNow we will clean up some more column names in preparation for graphing them\n\n\nRead and format data\n# Include and execute your code here\nspecific_column_names_mapping = {\n    'followingStarWarshaveseen': 'seenEpisode1',\n    'Unnamed4': 'seenEpisode2',\n    'Unnamed5': 'seenEpisode3',\n    'Unnamed6': 'seenEpisode4',\n    'Unnamed7': 'seenEpisode5',\n    'Unnamed8': 'seenEpisode6'\n}\ndata.rename(columns=specific_column_names_mapping, inplace=True)\n\nseen_counts = {\n    'seenEpisode1': data['seenEpisode1'].value_counts(),\n    'seenEpisode2': data['seenEpisode2'].value_counts(),\n    'seenEpisode3': data['seenEpisode3'].value_counts(),\n    'seenEpisode4': data['seenEpisode4'].value_counts(),\n    'seenEpisode5': data['seenEpisode5'].value_counts(),\n    'seenEpisode6': data['seenEpisode6'].value_counts()\n}\ntotal_respondents = data['Haveseen6StarWars'].value_counts().get('Yes')\n\nseen_counts_values = list(map(lambda x: x.iloc[0], seen_counts.values()))\n\n# Movie titles for the x-axis\nmovie_titles = [\n    'The Phantom Menace', \n    'Attack of the Clones', \n    'Revenge of the Sith', \n    'A New Hope', \n    'The Empire Strikes Back', \n    'Return of the Jedi'\n]\n\n# Calculate the percentage of respondents who have seen each movie\npercentages = [count / total_respondents * 100 for count in seen_counts_values]\n\n\n_Here are our results and how it compares to another company’s graph\n\nTheir graph:\n\n\n\nPercentage of respondents who have seen at least one movie’s responses\n\n\n\n\nOur Graph\n::: {#cell-Which star wars movies have you seen? .cell execution_count=5}\n\nplot example\n# Include and execute your code here\nplt.figure(figsize=(10, 6))\nplt.barh(movie_titles, percentages, color='darkblue')\nplt.xlabel('Percentage of Respondents')\nplt.title('Which ‘Star Wars’ Movies Have You Seen?')\nplt.xlim(0, 100)  # Set the x-axis limit to 0-100 for percentage\n\n# Add the percentages on the bars\nfor index, value in enumerate(percentages):\n    plt.text(value + 1, index, f'{value:.2f}%')  # Adjust the +1 if you need more offset from the bar\n\nplt.show()\n\n\n\n\n\nMy extremely useful chart\n\n\n\n:::\nNow we will compare our results to who shot first to their results of who shot first.\n\n\nTheir graph:\n\n\n\nWho shot first\n\n\n\n\nOur Graph\n\n\nplot example\n# Include and execute your code here\nwhich_shot_counts = data['Whichshot'].value_counts()\n\n# Calculate the total number of respondents\ntotal_respondents = data['Whichshot'].count()\n\n# Calculate the percentages\nwhich_shot_percentages = (which_shot_counts / total_respondents) * 100\n\n# Sort the index to ensure the order is correct when plotting\nsorted_index = ['Han', 'Greedo', \"I don't understand this question\"]\nwhich_shot_percentages = which_shot_percentages.reindex(sorted_index)\n\n\n::: {#cell-Who shot first .cell execution_count=7}\n\nPlot\n# Include and execute your code here\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars = ax.barh(which_shot_percentages.index, which_shot_percentages, color='darkblue')\n\n# Remove all spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\n# Remove x-axis\nax.xaxis.set_visible(False)\n\n# Remove y-axis tick marks\nax.tick_params(axis='y', which='both', left=False)\n\n# Set y-ticks and y-tick labels to be the character names\nax.set_yticks(range(len(sorted_index)))\nax.set_yticklabels(sorted_index)\n\n# Add the percentage labels to the right of the bars\nfor bar in bars:\n    ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{int(bar.get_width())}%', va='center', ha='right')\n\n# Set the title of the plot\nax.set_title('Who Shot First?', fontsize=14, fontweight='bold')\n\n# Add subtitle below the title\nax.text(0, 1.1, f'According to {total_respondents} respondents', transform=ax.transAxes, fontsize=10)\n\nplt.show()\n\n\n\n\n\nMy extremely useful chart\n\n\n\n:::\nNow we are going to clean up the rest of the data in preparation for our machine learning model\n\n\nCleaning the movies ranked, converting the age, education, and income categories into numbers, cleaning the ranking of characters, and one-hot encoding everything else\n\n\nRead and format data\n# Include and execute your code here\n\nranked_columns_mapping = {\n    'favoriterankStarWars16': 'rankedEp1',\n    'Unnamed10': 'rankedEp2',\n    'Unnamed11': 'rankedEp3',\n    'Unnamed12': 'rankedEp4',\n    'Unnamed13': 'rankedEp5',\n    'Unnamed14': 'rankedEp6'\n}\n\n# Rename the columns in the DataFrame\ndata.rename(columns=ranked_columns_mapping, inplace=True)\n\n\nstar_wars_films = data.iloc[:, 3:9].stack().dropna().reset_index(drop=True)\n\n# Rename the new column\nstar_wars_films.columns = ['seen']\n\n# Count the occurrences of each answer\nfilm_counts = star_wars_films.value_counts()\n\n#%%\n\nage_mapping = {\n    '18-29': 1,\n    '30-44': 2,\n    '45-60': 3,\n    '&gt; 60': 4\n}\n\n# Create a new column 'AgeNumeric' with the converted numeric age ranges\ndata['AgeNumeric'] = data['Age'].map(age_mapping)\n\n# Drop the original 'Age' column\ndata.drop('Age', axis=1, inplace=True)\n\n# Mapping education levels to numeric values\neducation_mapping = {\n    'Less than high school degree': 1,\n    'High school degree': 2,\n    'Some college or Associate degree': 3,\n    'Bachelor degree': 4,\n    'Graduate degree': 5\n}\n\n# Create a new column 'EducationNumeric' with the converted numeric education levels\ndata['EducationNumeric'] = data['Education'].map(education_mapping)\n\n# Drop the original 'Education' column\ndata.drop('Education', axis=1, inplace=True)\n\n\nincome_mapping = {\n    '$0 - $24,999': 1,\n    '$25,000 - $49,999': 2,\n    '$50,000 - $99,999': 3,\n    '$100,000 - $149,999': 4,\n    '$150,000+': 5\n}\n\n# Create a new column 'IncomeNumeric' with the converted numeric income ranges\ndata['IncomeNumeric'] = data['HouseholdIncome'].map(income_mapping)\n\n# Drop the original 'HouseholdIncome' column\ndata.drop('HouseholdIncome', axis=1, inplace=True)\n#%%\n\n\n\n#%%\n\nfilm_seen_columns = data.columns[1:7] \n\nfiltered_data = data[film_seen_columns].apply(lambda x: x == 'Yes').any(axis=1)\n\nseen_at_least_one_film = data[filtered_data]\n#%%\n\ncolumns_to_rename = [\n    'followingview', 'Unnamed16', 'Unnamed17', 'Unnamed18', 'Unnamed19', 'Unnamed20', \n    'Unnamed21', 'Unnamed22', 'Unnamed23', 'Unnamed24', 'Unnamed25', \n    'Unnamed26', 'Unnamed27', 'Unnamed28'\n]\n\nsentiment_mapping = {\n    'Very unfavorably': 1,\n    'Somewhat unfavorably': 2,\n    'Neither favorably nor unfavorably (neutral)': 3,\n    'Somewhat favorably': 4,\n    'Very favorably': 5,\n    'Unfamiliar (N/A)': 0  # Assuming 0 for 'Unfamiliar' if it should be included\n}\n\n# Rename columns based on the first row's content and keep track of the new names\nnew_names = []\nfor col in columns_to_rename:\n    new_col_name = data[col].iloc[0].split(',')[0].replace(' ', '')\n    data.rename(columns={col: new_col_name}, inplace=True)\n    new_names.append(new_col_name)\n\n# Remove the row where the column name is a data entry, using the new names\nfor new_col_name in new_names:\n    data = data[data[new_col_name] != new_col_name]\n\nfor col in new_names:  # Assuming 'new_names' contains the new column names\n    data[col] = data[col].map(sentiment_mapping)\n\n\n# Identify columns that are still of type object\nnon_numeric_columns = data.select_dtypes(include=['object']).columns\n\ncolumns_to_one_hot = [\n    'Haveseen6StarWars', 'considerfanStarWars',\n    'seenEpisode1', 'seenEpisode2', 'seenEpisode3', 'seenEpisode4',\n    'seenEpisode5', 'seenEpisode6', 'Whichshot',\n    'familiarAreExpandedUniverse', 'considerfanExpandedUniverse',\n    'considerfanStarTrek', 'Gender', 'CensusRegion', 'rankedEp1', 'rankedEp2', 'rankedEp3', 'rankedEp4',\n       'rankedEp5', 'rankedEp6', 'HanSolo', 'LukeSkywalker',\n       'PrincessLeiaOrgana', 'AnakinSkywalker', 'ObiWanKenobi',\n       'EmperorPalpatine', 'DarthVader', 'LandoCalrissian', 'BobaFett',\n       'C-3P0', 'R2D2', 'JarJarBinks', 'PadmeAmidala', 'Yoda'\n]\n\n# Apply one-hot encoding to the specified columns and update the original dataset\ndata = pd.get_dummies(data, columns=columns_to_one_hot)\n\n\nNow we will run our machine learning model. We are going to use the income categories of &gt;= to 3 to see if we can predict who makes more than $50,000 per year.\nWe discover here that with our best performing model, the gradient booster, education, age, and the respondent id account for 48.54% of the model’s accuracy. Our model was able to predict who makes over $50,000 per year with a 69% accuracy\n\n\nRunning the 3 different models\n# Include and execute your code here\n\ndata['Target'] = data['IncomeNumeric'] &gt;= 3\nX = data.drop(['IncomeNumeric', 'Target'], axis=1).fillna(0)\ny = data['Target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n}\n\nfeature_importances = {}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f'{name} Accuracy: {accuracy}')\n    \n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n    elif hasattr(model, 'coef_'):\n        importances = abs(model.coef_[0])\n    else:\n        importances = [0] * len(X.columns)  # Placeholder if the model does not have feature importances or coefficients\n    \n    feature_importances[name] = dict(zip(X.columns, importances))\n\n# Display the top 10 important features for each model\nfor name, importances in feature_importances.items():\n    print(f'\\n{name} - Top 10 Features:')\n    for feature, importance in sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f'{feature}: {importance:.4f}')\n\n\nRandom Forest Accuracy: 0.6386554621848739\nGradient Boosting Accuracy: 0.6932773109243697\nLogistic Regression Accuracy: 0.5630252100840336\n\nRandom Forest - Top 10 Features:\nRespondentID: 0.1104\nAgeNumeric: 0.0819\nEducationNumeric: 0.0789\nGender_Male: 0.0141\nconsiderfanStarTrek_No: 0.0139\nGender_Female: 0.0124\nCensusRegion_Pacific: 0.0109\nCensusRegion_East North Central: 0.0106\nconsiderfanStarTrek_Yes: 0.0103\nCensusRegion_South Atlantic: 0.0100\n\nGradient Boosting - Top 10 Features:\nEducationNumeric: 0.2535\nAgeNumeric: 0.1199\nRespondentID: 0.1112\nrankedEp6_3: 0.0287\nEmperorPalpatine_3.0: 0.0231\nCensusRegion_New England: 0.0171\nconsiderfanStarTrek_Yes: 0.0166\nBobaFett_2.0: 0.0135\nrankedEp5_3: 0.0128\nrankedEp6_2: 0.0125\n\nLogistic Regression - Top 10 Features:\nRespondentID: 0.0000\nEducationNumeric: 0.0000\nAgeNumeric: 0.0000\nseenEpisode5_Star Wars: Episode V The Empire Strikes Back: 0.0000\nseenEpisode6_Star Wars: Episode VI Return of the Jedi: 0.0000\nLukeSkywalker_5.0: 0.0000\nObiWanKenobi_5.0: 0.0000\nHanSolo_5.0: 0.0000\nPrincessLeiaOrgana_5.0: 0.0000\nseenEpisode4_Star Wars: Episode IV  A New Hope: 0.0000"
  },
  {
    "objectID": "ds250_project_template_clean (1).html",
    "href": "ds250_project_template_clean (1).html",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "The data shows that there are many factors that may or may not affect the popularity of a name. However, there is one clear trend among most names – they become increasingly popular until they taper off and fall back down to average levels. There are cultural groups of names that also seem to have correlations. It is important to note, however, that each case may be unique and different.\n\n\nRead and format project data\n# Include and execute your code here\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv'\n\n# Securely fetch the data using requests and certifi\nresponse = requests.get(url, verify=certifi.where())\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    df = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response.status_code)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "ds250_project_template_clean (1).html#elevator-pitch",
    "href": "ds250_project_template_clean (1).html#elevator-pitch",
    "title": "Client Report - Project 1",
    "section": "",
    "text": "The data shows that there are many factors that may or may not affect the popularity of a name. However, there is one clear trend among most names – they become increasingly popular until they taper off and fall back down to average levels. There are cultural groups of names that also seem to have correlations. It is important to note, however, that each case may be unique and different.\n\n\nRead and format project data\n# Include and execute your code here\nurl = 'https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv'\n\n# Securely fetch the data using requests and certifi\nresponse = requests.get(url, verify=certifi.where())\n\n# Check if the request was successful\nif response.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    df = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response.status_code)\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "ds250_project_template_clean (1).html#questiontask-1",
    "href": "ds250_project_template_clean (1).html#questiontask-1",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nMy name peaked the year before I was born. It followed the trend of many names, which is that it follows an average trend, becomes popular, then falls back down again.\n\n\nRead and format data\n# Include and execute your code here\nspencer_df = df.set_index('name').loc['Spencer']\n\nspencer_chart = px.bar(spencer_df,\n    x='year', \n    y='Total', \n    height=800, \n    width=800, \n    title='Usage of the name Spencer over time')\n\n# Add annotation for the year you were born\nspencer_chart.add_annotation(\n    x=1999,\n    y=spencer_df[spencer_df['year'] == 1999]['Total'].values[0],\n    text='When I was born',\n    showarrow=True,\n    arrowhead=1)\n\nspencer_chart.show()"
  },
  {
    "objectID": "ds250_project_template_clean (1).html#questiontask-2",
    "href": "ds250_project_template_clean (1).html#questiontask-2",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nBased on the results, I would guess that there is a high probability that Brittany was born between 1988 and 1993. However, the z scores of 1986-1994 are all above 1. It it the lowest probability that she was born after 2000 or before 1983. However, no z-score dips below -1, which means no year is significanly below average.\n\n\nRead and format data\n# Include and execute your code here\n\nbrittany_df = df[df['name'] == 'Brittany']\n\nmean_total = brittany_df['Total'].mean()\nstd_dev_total = brittany_df['Total'].std()\nbrittany_df['z_score'] = (brittany_df['Total'] - mean_total) / std_dev_total\n\ndef determine_color(z):\n    if z &gt; 1:\n        return 'Above 1'\n    elif z &lt; 0:\n        return 'Below 0'\n    else:\n        return 'Between 0 and 1'\n\nbrittany_df['color'] = brittany_df['z_score'].apply(determine_color)\n\n\nfig = px.bar(brittany_df,\n             x='year',\n             y='z_score',\n             color='color',\n             height=800,\n             width=800,\n             title='Z Score probability of the Name Brittany over time',\n             color_discrete_map={'Above 1': 'black', 'Below 0': 'red', 'Between 0 and 1': 'green' })\n\n# Find years where z_score &gt; 1.5\nhigh_z_score_years = brittany_df[brittany_df['z_score'] &gt; 1.5]['year']\n\nif not high_z_score_years.empty:\n    start_year = high_z_score_years.min()\n    end_year = high_z_score_years.max()\n\n    # Find the maximum Z score in the range and slightly adjust upwards\n    max_z_in_range = brittany_df[brittany_df['year'].between(start_year, end_year)]['z_score'].max()\n    bracket_top_y = max_z_in_range + 0.2  # Adjust the value to position the bracket just above the highest peak\n\n    # Draw horizontal line (top of the bracket)\n    fig.add_shape(\n        type=\"line\",\n        x0=start_year,\n        y0=bracket_top_y,\n        x1=end_year,\n        y1=bracket_top_y,\n        line=dict(\n            color=\"RoyalBlue\",\n            width=2,\n        ),\n    )\n\n    # Draw vertical line at the start year\n    fig.add_shape(\n        type=\"line\",\n        x0=start_year,\n        y0=bracket_top_y,\n        x1=start_year,\n        y1=bracket_top_y - 0.2,  # Adjust the length of the vertical line as needed\n        line=dict(\n            color=\"RoyalBlue\",\n            width=2,\n        ),\n    )\n\n    # Draw vertical line at the end year\n    fig.add_shape(\n        type=\"line\",\n        x0=end_year,\n        y0=bracket_top_y,\n        x1=end_year,\n        y1=bracket_top_y - 0.2,  # Adjust the length of the vertical line as needed\n        line=dict(\n            color=\"RoyalBlue\",\n            width=2,\n        ),\n    )\n\n    # Add annotation above the bracket\n    fig.add_annotation(\n        x=(start_year + end_year) / 2,\n        y=bracket_top_y + 0.3,  # Position the text above the bracket\n        text=\"The name Brittany was exceptionally popular these years\",\n        showarrow=False,\n        font=dict(\n            size=8,\n            color=\"RoyalBlue\"\n        )\n    )\n\nfig.show()"
  },
  {
    "objectID": "ds250_project_template_clean (1).html#questiontask-3",
    "href": "ds250_project_template_clean (1).html#questiontask-3",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nHere we see than in the 1960s, which is when there was many cultural movements in the United states, the usage of these names went down. They are all down significantly from their peak. It is unclear from the findings however, whether this is due to a decline in religion or if it is the same trend all the other names follow.\n\n\nRead and format data\n# Include and execute your code here\nnm = [\"Mary\", \"Martha\", \"Peter\", \"Paul\"]\n\nnm_data = df.query(\"name == @nm\").filter([\"name\", \"Total\", \"year\"])\n\nfig = px.line(nm_data, x=\"year\", y=\"Total\", color=\"name\", title=\"Christian Names Over Time\", height=1200, width=800)\n\nfig.update_layout(xaxis_range=[1920, 2000])\n\nfig.add_vline(x=1963, line_width=2, line_dash=\"dash\", line_color=\"red\")\n\nfig.add_annotation(x=1963, y=max(nm_data[\"Total\"]), text=\"Start of the feminist movement\",\n                   showarrow=True, arrowhead=1, ax=-50, ay=-100)\n\nfor name in nm:\n    data_1963 = nm_data[(nm_data['year'] == 1963) & (nm_data['name'] == name)]\n    data_2000 = nm_data[(nm_data['year'] == 2000) & (nm_data['name'] == name)]\n\n    if not data_1963.empty and not data_2000.empty:\n        total_1963 = data_1963['Total'].values[0]\n        total_2000 = data_2000['Total'].values[0]\n        percent_decrease = ((total_1963 - total_2000) / total_1963) * 100\n\n        # Calculate midpoint for annotation\n        mid_x = 1963 + (2000 - 1963) / 2\n        mid_y = total_1963 - (total_1963 * percent_decrease / 100) / 2\n\n        # Add line for percent decrease\n        fig.add_shape(type=\"line\",\n                      x0=1963, y0=total_1963,\n                      x1=2000, y1=total_1963 - (total_1963 * percent_decrease / 100),\n                      line=dict(color=\"black\", width=2, dash=\"dot\"))\n\n        # Add annotation at the midpoint of the line\n        fig.add_annotation(\n            x=mid_x, y=mid_y,\n            text=f\"{percent_decrease:.2f}% decrease\",\n            showarrow=False,\n            xshift=20,\n            font_size=8,\n            bgcolor=\"white\"\n        )\n\n# Show the figure\nfig.show()"
  },
  {
    "objectID": "ds250_project_template_clean (1).html#questiontask-4",
    "href": "ds250_project_template_clean (1).html#questiontask-4",
    "title": "Client Report - Project 1",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nIt is impossible to say whether or not the movie E.T. affected the use of the name Elliot based on the current data. It did increase after the movie. But it is unclear whether the movie made the name Elliot popular, if they used the name because it was already getting popular, or if there is no direct causal link at all.\n\n\nRead and format data\n# Include and execute your code here\n\nname = \"Elliot\"\net_release = 1982\n\net_df = df.query(\"name == @name\").filter([\"name\", \"Total\", \"year\"])\n\net_movie = px.line(et_df, x=\"year\", y=\"Total\", color=\"name\", title=\"Elliot Name Usage and E.T. Release\", height=800, width=800)\n\net_movie.add_bar(x=[et_release], y=[et_df.query(\"year == @et_release\")[\"Total\"].values[0]], name=\"E.T. Release\")\n\net_movie.add_annotation(\n    x=et_release,\n    y=et_df.query(\"year == @et_release\")[\"Total\"].values[0],\n    text=\"E.T. Release\",\n    showarrow=True,\n    arrowhead=1)\n\net_movie.show()"
  },
  {
    "objectID": "ds250_project_template_clean (4).html",
    "href": "ds250_project_template_clean (4).html",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "",
    "text": "Using this model will be essential for promoting the safety of individuals, especially when working with data that the year of the house built is unavailable. This is essential for those who might be sentitive to materials used to be built in older homes, and those who was to exercise an abundance of caution towards materials such as lead paint and aspestous.\nWe were able to build a reliable model with 95.9% accuracy. It uses the random tree classifier to analyze the data to make predictions about which houses were built before 1980. Parting out the parcel data was an important step in getting the last few increments of accuracy. Live area was another important piece in detecting old houses.\n\n\nShow the code\n# Securely fetch the data using requests and certifi\nurl1 = 'https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_denver/dwellings_denver.csv'\nurl2 = 'https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv'\n\nresponse1 = requests.get(url1, verify=certifi.where())\nresponse2 = requests.get(url2, verify=certifi.where())\n\n# Check if the request was successful\nif response1.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    denver_data = pd.read_csv(io.StringIO(response1.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response1.status_code)\nif response2.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    ml_data = pd.read_csv(io.StringIO(response2.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response2.status_code)\n\nvariable_description = {\n    'parcel': 'Character: The parcel id',\n    'abstrprd': 'Numeric: No clue',\n    'livearea': 'Numeric: Square footage that is liveable',\n    'finbsmnt': 'Numeric: Square footage finished in the basement',\n    'basement': 'Numeric: Total square footage of the basement',\n    'yrbuilt': 'Numeric: Year the home was built',\n    'totunits': 'Numeric: How many dwelling units in the building',\n    'stories': 'Numeric: The number of stories',\n    'nocars': 'Numeric: Size of the garage in cars',\n    'numbdrm': 'Numeric: Number of bedrooms',\n    'numbaths': 'Numeric: Number of bathrooms',\n    'sprice': 'Numeric: Selling price',\n    'deduct': 'Numeric: Deduction from the selling price',\n    'netprice': 'Numeric: Net price of home',\n    'tasp': 'Numeric: Tax assessed selling price',\n    'smonth': 'Numeric: Month sold',\n    'syear': 'Numeric: Year sold',\n    'condition_AVG': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Excel': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Fair': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Good': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_VGood': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_A': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_B': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_C': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_D': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_X': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Att': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Att/Det': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Det': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_None': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_att/CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_det/CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_BI-LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_CONVERSIONS': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_END UNIT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_MIDDLE UNIT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_ONE AND HALF-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_ONE-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_SPLIT LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_THREE-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TRI-LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TRI-LEVEL WITH BASEMENT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TWO AND HALF-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TWO-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'qualified_Q': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'qualified_U': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'status_I': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'status_V': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'before1980': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'book_num': 'Numeric: The first part of the parcel id',\n    'page_num': 'Numeric: The second part of the parcel number',\n    'parcel_num': 'Numeric: The part of the parcel number related to their portion of land',\n    'unit_num': 'Numeric: The part of the parcel related to the unit number.'\n}\n\n\nHere we will explore the relationship between homes built before 1980 and different aspects of our data. This will help us discover what metrics will be useful for our model."
  },
  {
    "objectID": "ds250_project_template_clean (4).html#safety-for-your-family",
    "href": "ds250_project_template_clean (4).html#safety-for-your-family",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "",
    "text": "Using this model will be essential for promoting the safety of individuals, especially when working with data that the year of the house built is unavailable. This is essential for those who might be sentitive to materials used to be built in older homes, and those who was to exercise an abundance of caution towards materials such as lead paint and aspestous.\nWe were able to build a reliable model with 95.9% accuracy. It uses the random tree classifier to analyze the data to make predictions about which houses were built before 1980. Parting out the parcel data was an important step in getting the last few increments of accuracy. Live area was another important piece in detecting old houses.\n\n\nShow the code\n# Securely fetch the data using requests and certifi\nurl1 = 'https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_denver/dwellings_denver.csv'\nurl2 = 'https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv'\n\nresponse1 = requests.get(url1, verify=certifi.where())\nresponse2 = requests.get(url2, verify=certifi.where())\n\n# Check if the request was successful\nif response1.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    denver_data = pd.read_csv(io.StringIO(response1.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response1.status_code)\nif response2.status_code == 200:\n    # Read the content of the response into a pandas DataFrame\n    ml_data = pd.read_csv(io.StringIO(response2.content.decode('utf-8')))\n    # Print the first 5 rows of the data\nelse:\n    print(\"Failed to retrieve data: Status code\", response2.status_code)\n\nvariable_description = {\n    'parcel': 'Character: The parcel id',\n    'abstrprd': 'Numeric: No clue',\n    'livearea': 'Numeric: Square footage that is liveable',\n    'finbsmnt': 'Numeric: Square footage finished in the basement',\n    'basement': 'Numeric: Total square footage of the basement',\n    'yrbuilt': 'Numeric: Year the home was built',\n    'totunits': 'Numeric: How many dwelling units in the building',\n    'stories': 'Numeric: The number of stories',\n    'nocars': 'Numeric: Size of the garage in cars',\n    'numbdrm': 'Numeric: Number of bedrooms',\n    'numbaths': 'Numeric: Number of bathrooms',\n    'sprice': 'Numeric: Selling price',\n    'deduct': 'Numeric: Deduction from the selling price',\n    'netprice': 'Numeric: Net price of home',\n    'tasp': 'Numeric: Tax assessed selling price',\n    'smonth': 'Numeric: Month sold',\n    'syear': 'Numeric: Year sold',\n    'condition_AVG': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Excel': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Fair': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_Good': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'condition_VGood': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_A': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_B': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_C': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_D': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'quality_X': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Att': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Att/Det': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_Det': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_None': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_att/CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'gartype_det/CP': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_BI-LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_CONVERSIONS': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_END UNIT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_MIDDLE UNIT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_ONE AND HALF-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_ONE-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_SPLIT LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_THREE-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TRI-LEVEL': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TRI-LEVEL WITH BASEMENT': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TWO AND HALF-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'arcstyle_TWO-STORY': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'qualified_Q': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'qualified_U': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'status_I': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'status_V': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'before1980': 'Numeric: 0 or 1 to mark building having attribute as 1',\n    'book_num': 'Numeric: The first part of the parcel id',\n    'page_num': 'Numeric: The second part of the parcel number',\n    'parcel_num': 'Numeric: The part of the parcel number related to their portion of land',\n    'unit_num': 'Numeric: The part of the parcel related to the unit number.'\n}\n\n\nHere we will explore the relationship between homes built before 1980 and different aspects of our data. This will help us discover what metrics will be useful for our model."
  },
  {
    "objectID": "ds250_project_template_clean (4).html#graph-live-area-and-year-built",
    "href": "ds250_project_template_clean (4).html#graph-live-area-and-year-built",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "Graph: Live area and year built:",
    "text": "Graph: Live area and year built:\nHere we see the live area density of homes built before 1980 peaks in a different area as compared to homes built after 1980 \n::: {#cell-Kernel density graph of the relationship between homes built before 1980 and live area. We limited it to 6000 square feet to control for outliers and to help make the graph more readable. .cell execution_count=3}\n\nShow the code\nlivearea_range = (0, 6000)\nplt.figure(figsize=(10, 6))\nsns.kdeplot(\n    data=ml_data,\n    x='livearea',\n    hue='before1980',\n    common_norm=False,\n    clip=livearea_range\n)\nplt.title('Density of Buildings by Live Area and Year Built')\nplt.xlabel('Live Area (sq ft)')\nplt.ylabel('Density')\nplt.xlim(livearea_range)  # Limit the x-axis to the range of interest\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "ds250_project_template_clean (4).html#graph-heatmap-for-the-correlation-between-parcel-data-and-year-built",
    "href": "ds250_project_template_clean (4).html#graph-heatmap-for-the-correlation-between-parcel-data-and-year-built",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "Graph: Heatmap for the correlation between parcel data and year built",
    "text": "Graph: Heatmap for the correlation between parcel data and year built\nWe discover that there is a strong correlation between parcel number and parcel units.\n\n\nthe heatmap of our data\ndwellings_ml = ml_data.copy()\n\n# Split the 'parcel' column into separate parts and expand into separate columns\nparcel_parts = dwellings_ml['parcel'].str.split('-', expand=True)\n\ndwellings_ml['book_num'] = pd.to_numeric(dwellings_ml['parcel'].str.split('-', expand=True)[0], errors='coerce')\ndwellings_ml['page_num'] = pd.to_numeric(dwellings_ml['parcel'].str.split('-', expand=True)[1], errors='coerce')\ndwellings_ml['parcel_num'] = pd.to_numeric(dwellings_ml['parcel'].str.split('-', expand=True)[2], errors='coerce')\ndwellings_ml['unit_num'] = pd.to_numeric(dwellings_ml['parcel'].str.split('-', expand=True)[3], errors='coerce')\n\n\ndwellings_ml['parcel_unit_interaction'] = pd.to_numeric(dwellings_ml['parcel_num'] * dwellings_ml['unit_num'], errors='coerce')\n\n\nfeatures_to_correlate = ['book_num', 'page_num', 'parcel_num', 'unit_num', 'yrbuilt']\ncorrelation_matrix = dwellings_ml[features_to_correlate].corr()\n\n# Plotting the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\nplt.title('Correlation Heatmap Including Unit Numbers')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)  \nplt.show()"
  },
  {
    "objectID": "ds250_project_template_clean (4).html#the-best-man-for-the-job-random-forest-classifier",
    "href": "ds250_project_template_clean (4).html#the-best-man-for-the-job-random-forest-classifier",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "The best man for the job: Random Forest Classifier",
    "text": "The best man for the job: Random Forest Classifier\nIncluded for the model are all rows of data. We let the program decide which data is significant and which data isn’t. In addition, we turned the parcel number into more useful pieces of data. I chose this model because I ran code to simultaneously test all the models, and this is the one with the best results. \n\n\nShow the code\nX = dwellings_ml.drop(columns=['yrbuilt', 'before1980', 'parcel'])\ny = dwellings_ml['before1980']\n\nX = pd.get_dummies(X, drop_first=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Best parameters found by GridSearchCV\nbest_params = {'max_depth': None, \n               'min_samples_leaf': 1, \n               'min_samples_split': 5, \n               'n_estimators': 200}\n\n# Initialize the RandomForestClassifier with the best parameters\nrfc_optimized = RandomForestClassifier(max_depth=best_params['max_depth'],\n    min_samples_leaf=best_params['min_samples_leaf'],\n    min_samples_split=best_params['min_samples_split'],\n    n_estimators=best_params['n_estimators'],\n    random_state=42)  # It's good practice to set a random_state for reproducibility\n\n# Now, you can fit this model to your training data\nrfc_optimized.fit(X_train, y_train)\n\n# And make predictions or evaluate its performance\ny_pred = rfc_optimized.predict(X_test)\n\n# Evaluate the model\n\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test set accuracy with optimized parameters: {test_accuracy:.4f}\")\n\n\nTest set accuracy with optimized parameters: 0.9594\n\n\nBook number appears to be the most important determiner when learning whether a house was built before 1980.\n\n\nShow the code\nfeature_importances = rfc_optimized.feature_importances_\nimportance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n\n# Merge importance_df with variable_description\nimportance_df = importance_df.merge(pd.DataFrame.from_dict(variable_description, orient='index', columns=['Description']),\n    left_on='Feature', right_index=True, how='left')\n\n# Sort the DataFrame by importance values in descending order\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\ntop_n = 10  # You can adjust this value to print more or fewer features\nprint(\"Top {} most important features:\".format(top_n))\nprint(tabulate(importance_df.head(top_n), headers='keys', tablefmt='fancy_grid'))\n\n\nTop 10 most important features:\n╒════╤════════════════════╤══════════════╤═════════════════════════════════════════════════════════════════════════╕\n│    │ Feature            │   Importance │ Description                                                             │\n╞════╪════════════════════╪══════════════╪═════════════════════════════════════════════════════════════════════════╡\n│ 48 │ book_num           │    0.171594  │ Numeric: The first part of the parcel id                                │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│ 37 │ arcstyle_ONE-STORY │    0.0673221 │ Numeric: 0 or 1 to mark building having attribute as 1                  │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│  5 │ stories            │    0.0585926 │ Numeric: The number of stories                                          │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│  1 │ livearea           │    0.0565069 │ Numeric: Square footage that is liveable                                │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│  8 │ numbaths           │    0.0503972 │ Numeric: Number of bathrooms                                            │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│ 22 │ quality_C          │    0.0472272 │ Numeric: 0 or 1 to mark building having attribute as 1                  │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│ 25 │ gartype_Att        │    0.0438935 │ Numeric: 0 or 1 to mark building having attribute as 1                  │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│ 12 │ tasp               │    0.0372386 │ Numeric: Tax assessed selling price                                     │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│ 50 │ parcel_num         │    0.0361268 │ Numeric: The part of the parcel number related to their portion of land │\n├────┼────────────────────┼──────────────┼─────────────────────────────────────────────────────────────────────────┤\n│  9 │ sprice             │    0.0327785 │ Numeric: Selling price                                                  │\n╘════╧════════════════════╧══════════════╧═════════════════════════════════════════════════════════════════════════╛"
  },
  {
    "objectID": "ds250_project_template_clean (4).html#justification-of-metrics",
    "href": "ds250_project_template_clean (4).html#justification-of-metrics",
    "title": "Client Report - Using Machine Learning to Predict Which Houses are Old",
    "section": "Justification of metrics:",
    "text": "Justification of metrics:\nPrecision: Imagine you’re trying to guess which houses were built before 1980. Precision is like being precise with your guesses. If your precision is high, it means most of the houses you guessed were built before 1980 are actually correct. So, if you say, “I think this house was built before 1980,” you’re usually right.\nRecall: Recall is like not missing any houses that were actually built before 1980. If your recall is high, it means you’re good at spotting all the houses that were built before 1980. So, you don’t miss many old houses when you’re guessing which ones are old.\nF1 Score: The F1 score combines both precision and recall. It’s like looking at both how often you’re right with your guesses and how many old houses you manage to find. A high F1 score means you’re both accurate with your guesses and you don’t miss many old houses. It’s like hitting a sweet spot between being right and not missing anything.\n\n\nShow the code\n## Calculate the precision\nprecision = precision_score(y_test, y_pred)\n\n# Calculate the recall\nrecall = recall_score(y_test, y_pred)\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(\"Precision:\", round(precision, 4))\nprint(\"Recall:\", round(recall, 4))\nprint(\"F1 Score:\", round(f1, 4))\n\n\nPrecision: 0.9663\nRecall: 0.969\nF1 Score: 0.9677"
  },
  {
    "objectID": "ds250_project_template.html",
    "href": "ds250_project_template.html",
    "title": "Client Report - Project 0: Introduction",
    "section": "",
    "text": "THIS .qmd FILE SHOULD BE USED TO WRITE YOUR REPORT. YOU WILL NEED TO COMPILE THE REPORT INTO A .html DOCUMENT AND SUBMIT IT ON CANVAS."
  },
  {
    "objectID": "ds250_project_template.html#elevator-pitch",
    "href": "ds250_project_template.html#elevator-pitch",
    "title": "Client Report - Project 0: Introduction",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nHere we learn that larger vehicles tend to have lower gas milage. It is an important purchasing consideration for both enviornmental and economic purposes. It confirms the hypothesis that large engines tend to use more fuel. It begs the question on whether companies should aim for smaller engines.\n\n\nRead and format project data\n# Include and execute your code here\nmpg = pd.read_csv('mpg.csv')\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "ds250_project_template.html#questiontask-1",
    "href": "ds250_project_template.html#questiontask-1",
    "title": "Client Report - Project 0: Introduction",
    "section": "Question|Task 1",
    "text": "Question|Task 1\n\n\nRead and format data\nprint(mpg\n  .head(5)\n  .filter([\"manufacturer\", \"model\",\"year\", \"hwy\"])\n  .to_markdown(index=False))\n\n\n| manufacturer   | model   |   year |   hwy |\n|:---------------|:--------|-------:|------:|\n| audi           | a4      |   1999 |    29 |\n| audi           | a4      |   1999 |    29 |\n| audi           | a4      |   2008 |    31 |\n| audi           | a4      |   2008 |    30 |\n| audi           | a4      |   1999 |    26 |\n\n\nHere we break down the mpg for each car based on its year.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot of data\npx.scatter(mpg, x=\"displ\", y=\"hwy\", color=\"manufacturer\")\n\n\n                                                \nMPG Chart\n\n:::"
  }
]